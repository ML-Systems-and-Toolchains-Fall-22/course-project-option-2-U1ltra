{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14-813/18-813 Course Project â€“ Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-I: Build and populate necessary tables\n",
    "\n",
    "Firstly, I contruct the spark section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/25 22:07:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# if you installed Spark on windows, \n",
    "# you may need findspark and need to initialize it prior to being able to use pyspark\n",
    "# Also, you may need to initialize SparkContext yourself.\n",
    "# Uncomment the following lines if you are using Windows!\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "#findspark.find()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GenericAppName\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, I import the MQTT training and test dataset and add a new column to identify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "\n",
    "col_names = [\n",
    "    \"tcp.flags\",\n",
    "    \"tcp.time_delta\",\n",
    "    \"tcp.len\",\n",
    "    \"mqtt.conack.flags\",\n",
    "    \"mqtt.conack.flags.reserved\",\n",
    "    \"mqtt.conack.flags.sp\",\n",
    "    \"mqtt.conack.val\",\n",
    "    \"mqtt.conflag.cleansess\",\n",
    "    \"mqtt.conflag.passwd\",\n",
    "    \"mqtt.conflag.qos\",\n",
    "    \"mqtt.conflag.reserved\",\n",
    "    \"mqtt.conflag.retain\",\n",
    "    \"mqtt.conflag.uname\",\n",
    "    \"mqtt.conflag.willflag\",\n",
    "    \"mqtt.conflags\",\n",
    "    \"mqtt.dupflag\",\n",
    "    \"mqtt.hdrflags\",\n",
    "    \"mqtt.kalive\",\n",
    "    \"mqtt.len\",\n",
    "    \"mqtt.msg\",\n",
    "    \"mqtt.msgid\",\n",
    "    \"mqtt.msgtype\",\n",
    "    \"mqtt.proto_len\",\n",
    "    \"mqtt.protoname\",\n",
    "    \"mqtt.qos\",\n",
    "    \"mqtt.retain\",\n",
    "    \"mqtt.sub.qos\",\n",
    "    \"mqtt.suback.qos\",\n",
    "    \"mqtt.ver\",\n",
    "    \"mqtt.willmsg\",\n",
    "    \"mqtt.willmsg_len\",\n",
    "    \"mqtt.willtopic\",\n",
    "    \"mqtt.willtopic_len\",\n",
    "    \"target\"\n",
    "]\n",
    "\n",
    "#print (col_names.tolist())\n",
    "\n",
    "# train_path = \"/Users/ultrali/Documents/Grad/14813/project/archive/Data/FINAL_CSV/train70_augmented.csv\"\n",
    "# test_path = \"/Users/ultrali/Documents/Grad/14813/project/archive/Data/FINAL_CSV/test30_augmented.csv\"\n",
    "train_path = \"/Users/ultrali/Documents/Grad/14813/project/archive/Data/FINAL_CSV/train70_reduced.csv\"\n",
    "test_path = \"/Users/ultrali/Documents/Grad/14813/project/archive/Data/FINAL_CSV/test30_reduced.csv\"\n",
    "\n",
    "df_train = spark.read.csv(train_path, header=True, inferSchema= True).toDF(*col_names)\n",
    "df_test = spark.read.csv(test_path, header=True, inferSchema= True).toDF(*col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.withColumn(\"dataset\", lit(0))\n",
    "df_test = df_test.withColumn(\"dataset\", lit(1))\n",
    "\n",
    "df_train.show(1, vertical=True)\n",
    "df_test.show(1, vertical=True)\n",
    "\n",
    "print(df_train.count())\n",
    "print(df_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the data into postgre table through pyspark jdbc API and verify the result with read command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_properties={}\n",
    "#update your db username\n",
    "db_properties['username']=\"14813\"\n",
    "#update your db password\n",
    "db_properties['password']=\"bigdata\"\n",
    "#make sure you got the right port number here\n",
    "db_properties['url']= \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "#make sure you had the Postgres JAR file in the right location\n",
    "db_properties['driver']=\"org.postgresql.Driver\"\n",
    "db_properties['table']= \"mqtt\"\n",
    "\n",
    "\n",
    "df_train.write.format(\"jdbc\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", db_properties['password'])\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()\n",
    "\n",
    "df_test.write.format(\"jdbc\")\\\n",
    ".mode(\"append\")\\\n",
    ".option(\"url\", db_properties['url'])\\\n",
    ".option(\"dbtable\", db_properties['table'])\\\n",
    ".option(\"user\", db_properties['username'])\\\n",
    ".option(\"password\", db_properties['password'])\\\n",
    ".option(\"Driver\", db_properties['driver'])\\\n",
    ".save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "df_read = sqlContext.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", db_properties['url'])\\\n",
    "    .option(\"dbtable\", db_properties['table'])\\\n",
    "    .option(\"user\", db_properties['username'])\\\n",
    "    .option(\"password\", db_properties['password'])\\\n",
    "    .option(\"Driver\", db_properties['driver'])\\\n",
    "    .load()\n",
    "\n",
    "df_read.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
