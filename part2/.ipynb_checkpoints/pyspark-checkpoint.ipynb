{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\n",
    "    'tcp_flags',\n",
    "    'tcp_time_delta',\n",
    "    'tcp_len',\n",
    "    'mqtt_conack_flags',\n",
    "    'mqtt_conack_flags_reserved',\n",
    "    'mqtt_conack_flags_sp',\n",
    "    'mqtt_conack_val',\n",
    "    'mqtt_conflag_cleansess',\n",
    "    'mqtt_conflag_passwd',\n",
    "    'mqtt_conflag_qos',\n",
    "    'mqtt_conflag_reserved',\n",
    "    'mqtt_conflag_retain',\n",
    "    'mqtt_conflag_uname',\n",
    "    'mqtt_conflag_willflag',\n",
    "    'mqtt_conflags',\n",
    "    'mqtt_dupflag',\n",
    "    'mqtt_hdrflags',\n",
    "    'mqtt_kalive',\n",
    "    'mqtt_len',\n",
    "    'mqtt_msg',\n",
    "    'mqtt_msgid',\n",
    "    'mqtt_msgtype',\n",
    "    'mqtt_proto_len',\n",
    "    'mqtt_protoname',\n",
    "    'mqtt_qos',\n",
    "    'mqtt_retain',\n",
    "    'mqtt_sub_qos',\n",
    "    'mqtt_suback_qos',\n",
    "    'mqtt_ver',\n",
    "    'mqtt_willmsg',\n",
    "    'mqtt_willmsg_len',\n",
    "    'mqtt_willtopic',\n",
    "    'mqtt_willtopic_len',\n",
    "    'target',\n",
    "    'dataset',\n",
    "    'tcp_flags_decimal',\n",
    "    'mqtt_conflags_decimal',\n",
    "    'mqtt_hdrflags_decimal'\n",
    "]\n",
    "\n",
    "nominal_cols = [\"mqtt_msg\"]\n",
    "\n",
    "corelated_cols_to_remove = [\"mqtt_conack_flags\",\n",
    "                     \"mqtt_conack_flags_reserved\",\n",
    "                     \"mqtt_conack_flags_sp\", \n",
    "                     \"mqtt_conflag_qos\", \n",
    "                     \"mqtt_conflag_reserved\", \n",
    "                     \"mqtt_conflag_retain\", \n",
    "                     \"mqtt_conflag_willflag\", \n",
    "                     \"mqtt_willtopic\", \n",
    "                     \"mqtt_willtopic_len\", \n",
    "                     \"mqtt_sub_qos\", \n",
    "                     \"mqtt_suback_qos\", \n",
    "                     \"mqtt_willmsg\",\n",
    "                     \"mqtt_willmsg_len\"]\n",
    "\n",
    "\n",
    "\n",
    "binary_cols = [\"mqtt_conack_val\", \n",
    "                  \"mqtt_conflag_cleansess\", \n",
    "                  \"mqtt_conflag_passwd\", \n",
    "                  \"mqtt_conflag_uname\", \n",
    "                  \"mqtt_dupflag\", \n",
    "                  \"mqtt_proto_len\", \n",
    "                  \"mqtt_protoname\", \n",
    "                  \"mqtt_qos\", \n",
    "                  \"mqtt_retain\", \n",
    "                  \"mqtt_ver\"]\n",
    "\n",
    "\n",
    "continuous_cols = [\"tcp_flags_decimal\",\n",
    "                     'tcp_time_delta',\n",
    "                     'tcp_len',\n",
    "                     'mqtt_kalive',\n",
    "                     'mqtt_len',\n",
    "                     'mqtt_msgid',\n",
    "                     'mqtt_msgtype',\n",
    "                     'mqtt_conflags_decimal', \n",
    "                     \"mqtt_hdrflags_decimal\", ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mqtt_msg']\n",
      "['mqtt_msg_index']\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mprint\u001b[39m(nominal_cols)\n\u001b[1;32m     79\u001b[0m \u001b[39mprint\u001b[39m(nominal_id_cols)\n\u001b[0;32m---> 80\u001b[0m stage_nominal_indexer \u001b[39m=\u001b[39m StringIndexer(inputCols \u001b[39m=\u001b[39;49m nominal_cols, outputCols \u001b[39m=\u001b[39;49m nominal_id_cols )\n\u001b[1;32m     82\u001b[0m \u001b[39m# Stage where the index columns are further transformed using OneHotEncoder\u001b[39;00m\n\u001b[1;32m     83\u001b[0m stage_nominal_onehot_encoder \u001b[39m=\u001b[39m OneHotEncoder(inputCols\u001b[39m=\u001b[39mnominal_id_cols, outputCols\u001b[39m=\u001b[39mnominal_onehot_cols)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/__init__.py:135\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMethod \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m forces keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_kwargs \u001b[39m=\u001b[39m kwargs\n\u001b[0;32m--> 135\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/ml/feature.py:4650\u001b[0m, in \u001b[0;36mStringIndexer.__init__\u001b[0;34m(self, inputCol, outputCol, inputCols, outputCols, handleInvalid, stringOrderType)\u001b[0m\n\u001b[1;32m   4645\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4646\u001b[0m \u001b[39m__init__(self, \\\\*, inputCol=None, outputCol=None, inputCols=None, outputCols=None, \\\u001b[39;00m\n\u001b[1;32m   4647\u001b[0m \u001b[39m         handleInvalid=\"error\", stringOrderType=\"frequencyDesc\")\u001b[39;00m\n\u001b[1;32m   4648\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4649\u001b[0m \u001b[39msuper\u001b[39m(StringIndexer, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m-> 4650\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_java_obj(\u001b[39m\"\u001b[39;49m\u001b[39morg.apache.spark.ml.feature.StringIndexer\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muid)\n\u001b[1;32m   4651\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_kwargs\n\u001b[1;32m   4652\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msetParams(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:80\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39mReturns a new Java object.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n\u001b[0;32m---> 80\u001b[0m \u001b[39massert\u001b[39;00m sc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     82\u001b[0m java_obj \u001b[39m=\u001b[39m _jvm()\n\u001b[1;32m     83\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m java_class\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.ml import Pipeline,Transformer\n",
    "from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class OutcomeCreater(Transformer): # this defines a transformer that creates the outcome column\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        # label_to_binary = udf(lambda name: 0.0 if name == 'normal' else 1.0)\n",
    "        # output_df = dataset.withColumn('outcome', label_to_binary(col('class'))).drop(\"class\")  \n",
    "        # output_df = output_df.withColumn('outcome', col('outcome').cast(DoubleType()))\n",
    "        # output_df = output_df.drop('difficulty')\n",
    "        # return output_df\n",
    "\n",
    "        def converter(name):\n",
    "            if name == 'legitimate':\n",
    "                return 0\n",
    "            elif name == 'dos':\n",
    "                return 1      \n",
    "            elif name == 'malformed':\n",
    "                return 2     \n",
    "            elif name == 'flood':\n",
    "                return 3\n",
    "            elif name == 'bruteforce':\n",
    "                return 4   \n",
    "            elif name == 'slowite':\n",
    "                return 5  \n",
    "            else:\n",
    "                print(\"out of range\")\n",
    "                return None\n",
    "            \n",
    "        label_to_binary = udf(lambda name: converter(name))\n",
    "        output_df = dataset.withColumn('label', label_to_binary(col('target')))\n",
    "        output_df = output_df.drop(\"target\")  \n",
    "        output_df = output_df.withColumn('label', col('label').cast(DoubleType()))\n",
    "        return output_df\n",
    "\n",
    "class FeatureTypeCaster(Transformer): # this transformer will cast the columns as appropriate types  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in binary_cols + continuous_cols:\n",
    "            output_df = output_df.withColumn(col_name,col(col_name).cast(DoubleType()))\n",
    "\n",
    "        return output_df\n",
    "class ColumnDropper(Transformer): # this transformer drops unnecessary columns\n",
    "    def __init__(self, columns_to_drop = None):\n",
    "        super().__init__()\n",
    "        self.columns_to_drop=columns_to_drop\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in self.columns_to_drop:\n",
    "            output_df = output_df.drop(col_name)\n",
    "        return output_df\n",
    "\n",
    "# Stage where columns are casted as appropriate types\n",
    "stage_typecaster = FeatureTypeCaster()\n",
    "\n",
    "# Stage where nominal columns are transformed to index columns using StringIndexer\n",
    "nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n",
    "nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n",
    "print(nominal_cols)\n",
    "print(nominal_id_cols)\n",
    "stage_nominal_indexer = StringIndexer(inputCols = nominal_cols, outputCols = nominal_id_cols )\n",
    "\n",
    "# Stage where the index columns are further transformed using OneHotEncoder\n",
    "stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n",
    "\n",
    "# Stage where all relevant features are assembled into a vector (and dropping a few)\n",
    "feature_cols = continuous_cols+binary_cols+nominal_onehot_cols\n",
    "for col_name in corelated_cols_to_remove:\n",
    "    feature_cols.remove(col_name)\n",
    "stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\")\n",
    "\n",
    "# Stage where we scale the columns\n",
    "stage_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n",
    "\n",
    "\n",
    "# Stage for creating the outcome column representing whether there is attack \n",
    "stage_outcome = OutcomeCreater()\n",
    "\n",
    "# Removing all unnecessary columbs, only keeping the 'features' and 'outcome' columns\n",
    "stage_column_dropper = ColumnDropper(columns_to_drop = nominal_cols+nominal_id_cols+\n",
    "    nominal_onehot_cols+ binary_cols + continuous_cols + ['vectorized_features'])\n",
    "\n",
    "\n",
    "# Estimator\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'outcome')\n",
    "\n",
    "# ParameterGrid\n",
    "lr_paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())\n",
    "\n",
    "# Evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', \n",
    "    labelCol='outcome', metricName='areaUnderROC')\n",
    "\n",
    "# CrossValidator\n",
    "lr_cv = CrossValidator(estimator=lr, estimatorParamMaps=lr_paramGrid, \n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "\n",
    "\n",
    "# Connect the columns into a pipeline\n",
    "ml_pipeline = Pipeline(stages=[stage_typecaster,stage_nominal_indexer,stage_nominal_onehot_encoder,\n",
    "    stage_vector_assembler,stage_scaler,stage_outcome,stage_column_dropper,lr_cv])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
