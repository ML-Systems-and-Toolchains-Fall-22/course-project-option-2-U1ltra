{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d849cf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 21:49:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"ML_Model\").getOrCreate()\n",
    "# spark.conf.set(\"spark.executor.memory\", \"8g\")\n",
    "# spark.conf.set(\"spark.executor.cores\", \"4\")\n",
    "# spark.conf.set(\"spark.driver.cores\", \"4\")\n",
    "# conf = SparkConf().set(\"spark.jars\", \"/Users/sz904/Downloads/postgresql-42.2.18.jar\")\n",
    "# sc = SparkContext( conf=conf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f4ab97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline,Transformer\n",
    "from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ca43a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['tcp_flags',\n",
    " 'tcp_time_delta',\n",
    " 'tcp_len',\n",
    " 'mqtt_conack_flags',\n",
    " 'mqtt_conack_flags_reserved',\n",
    " 'mqtt_conack_flags_sp',\n",
    " 'mqtt_conack_val',\n",
    " 'mqtt_conflag_cleansess',\n",
    " 'mqtt_conflag_passwd',\n",
    " 'mqtt_conflag_qos',\n",
    " 'mqtt_conflag_reserved',\n",
    " 'mqtt_conflag_retain',\n",
    " 'mqtt_conflag_uname',\n",
    " 'mqtt_conflag_willflag',\n",
    " 'mqtt_conflags',\n",
    " 'mqtt_dupflag',\n",
    " 'mqtt_hdrflags',\n",
    " 'mqtt_kalive',\n",
    " 'mqtt_len',\n",
    " 'mqtt_msg',\n",
    " 'mqtt_msgid',\n",
    " 'mqtt_msgtype',\n",
    " 'mqtt_proto_len',\n",
    " 'mqtt_protoname',\n",
    " 'mqtt_qos',\n",
    " 'mqtt_retain',\n",
    " 'mqtt_sub_qos',\n",
    " 'mqtt_suback_qos',\n",
    " 'mqtt_ver',\n",
    " 'mqtt_willmsg',\n",
    " 'mqtt_willmsg_len',\n",
    " 'mqtt_willtopic',\n",
    " 'mqtt_willtopic_len',\n",
    " 'target',\n",
    " 'dataset',\n",
    " 'tcp_flags_decimal',\n",
    " 'mqtt_conflags_decimal',\n",
    " 'mqtt_hdrflags_decimal']\n",
    "train_path = \"/Users/sz904/Desktop/df_train_pandas.csv\"\n",
    "test_path = \"/Users/sz904/Desktop/df_test_pandas.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "88d9165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train_updated = spark.read.csv(train_path, header=True, inferSchema= True).toDF(*col_names)\n",
    "df_test_updated = spark.read.csv(test_path, header=True, inferSchema= True).toDF(*col_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "df02dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \"dataset\" is removed from the cols, since it is only a indicator for \n",
    "#  for the train/test\n",
    "df_train_updated = df_train_updated.drop(\"tcp_flags\")\n",
    "df_train_updated = df_train_updated.drop(\"mqtt_conflags\")\n",
    "df_train_updated = df_train_updated.drop(\"mqtt_hdrflags\")\n",
    "df_train_updated = df_train_updated.drop(\"dataset\")\n",
    "\n",
    "df_test_updated = df_test_updated.drop(\"tcp_flags\")\n",
    "df_test_updated = df_test_updated.drop(\"mqtt_conflags\")\n",
    "df_test_updated = df_test_updated.drop(\"mqtt_hdrflags\")\n",
    "df_test_updated = df_test_updated.drop(\"dataset\")\n",
    "\n",
    "\n",
    "df_train_updated = df_train_updated.drop(\"mqtt_msg\")\n",
    "df_test_updated = df_test_updated.drop(\"mqtt_msg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "60755da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Col_names = df_train_updated.columns\n",
    "'''\n",
    " \"dataset\" is removed from the cols, since it is only a indicator for \n",
    " for the train/test\n",
    "\n",
    "corelated_cols_to_remove are dropped since after looking into them, \n",
    "values are all zeros in train dataset. Since the model can't learn\n",
    "anything from it, and thus they should be removed. \n",
    "'''\n",
    "nominal_cols = [\"mqtt_msg\"]\n",
    "\n",
    "corelated_cols_to_remove = [\"mqtt_conack_flags\",\n",
    "                     \"mqtt_conack_flags_reserved\",\n",
    "                     \"mqtt_conack_flags_sp\", \n",
    "                     \"mqtt_conflag_qos\", \n",
    "                     \"mqtt_conflag_reserved\", \n",
    "                     \"mqtt_conflag_retain\", \n",
    "                     \"mqtt_conflag_willflag\", \n",
    "                     \"mqtt_willtopic\", \n",
    "                     \"mqtt_willtopic_len\", \n",
    "                     \"mqtt_sub_qos\", \n",
    "                     \"mqtt_suback_qos\", \n",
    "                     \"mqtt_willmsg\",\n",
    "                     \"mqtt_willmsg_len\"]\n",
    "\n",
    "\n",
    "\n",
    "binary_cols = [\"mqtt_conack_val\", \n",
    "                  \"mqtt_conflag_cleansess\", \n",
    "                  \"mqtt_conflag_passwd\", \n",
    "                  \"mqtt_conflag_uname\", \n",
    "                  \"mqtt_dupflag\", \n",
    "                  \"mqtt_proto_len\", \n",
    "                  \"mqtt_protoname\", \n",
    "                  \"mqtt_qos\", \n",
    "                  \"mqtt_retain\", \n",
    "                  \"mqtt_ver\"]\n",
    "\n",
    "\n",
    "continuous_cols = [\"tcp_flags_decimal\",\n",
    "                     'tcp_time_delta',\n",
    "                     'tcp_len',\n",
    "                     'mqtt_kalive',\n",
    "                     'mqtt_len',\n",
    "                     'mqtt_msgid',\n",
    "                     'mqtt_msgtype',\n",
    "                     'mqtt_conflags_decimal', \n",
    "                     \"mqtt_hdrflags_decimal\", ]\n",
    "\n",
    "len(nominal_cols+corelated_cols_to_remove+binary_cols+continuous_cols)\n",
    "# len(df_train_updated.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fa9b1244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Q1.1\n",
    "\n",
    "\n",
    "class OutcomeCreater(Transformer): # this defines a transformer that creates the outcome column\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        \n",
    "        def converter(name):\n",
    "            if name == 'legitimate':\n",
    "                return 0\n",
    "            elif name == 'dos':\n",
    "                return 1      \n",
    "            elif name == 'malformed':\n",
    "                return 2     \n",
    "            elif name == 'flood':\n",
    "                return 3\n",
    "            elif name == 'bruteforce':\n",
    "                return 4   \n",
    "            elif name == 'slowite':\n",
    "                return 5  \n",
    "            else:\n",
    "                print(\"out of range\")\n",
    "                return None\n",
    "            \n",
    "        label_to_binary = udf(lambda name: converter(name))\n",
    "        output_df = dataset.withColumn('label', label_to_binary(col('target')))\n",
    "        output_df = output_df.drop(\"target\")  \n",
    "        output_df = output_df.withColumn('label', col('label').cast('int'))\n",
    "        return output_df\n",
    "    \n",
    "class FeatureTypeCaster(Transformer): # this transformer will cast the columns as appropriate types  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in binary_cols + continuous_cols:\n",
    "            output_df = output_df.withColumn(col_name,col(col_name).cast('float'))\n",
    "        return output_df\n",
    "    \n",
    "class ColumnDropper(Transformer): # this transformer drops unnecessary columns\n",
    "    def __init__(self, columns_to_drop = None):\n",
    "        super().__init__()\n",
    "        self.columns_to_drop=columns_to_drop\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in self.columns_to_drop:\n",
    "            output_df = output_df.drop(col_name)\n",
    "        return output_df\n",
    "    \n",
    "    \n",
    "def get_preprocess_pipeline():\n",
    "\n",
    "    # Stage where columns are casted as appropriate types\n",
    "    stage_typecaster = FeatureTypeCaster()\n",
    "    \n",
    "    # Stage where nominal columns are transformed to index columns using StringIndexer\n",
    "#     nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n",
    "#     nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n",
    "#     stage_nominal_indexer = StringIndexer(inputCols = nominal_cols, outputCols = nominal_id_cols )\n",
    "\n",
    "    # Stage where the index columns are further transformed using OneHotEncoder\n",
    "#     stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n",
    "\n",
    "    # Stage where all relevant features are assembled into a vector (and dropping a few)\n",
    "    feature_cols = continuous_cols+binary_cols\n",
    "    \n",
    "#     drop_counter = 0\n",
    "    \n",
    "#     for col_name in corelated_cols_to_remove:\n",
    "#         feature_cols.remove(col_name)\n",
    "#         drop_counter += 1\n",
    "        \n",
    "#     print(drop_counter)\n",
    "\n",
    "#     stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\").setHandleInvalid(\"skip\")\n",
    "#     stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\").setHandleInvalid(\"skip\")\n",
    "#     stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\").setHandleInvalid(\"keep\")\n",
    "\n",
    "    # Stage where we scale the columns\n",
    "    \n",
    "#     stage_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n",
    "    \n",
    "\n",
    "    # Stage for creating the outcome column representing whether there is attack \n",
    "    stage_outcome = OutcomeCreater()\n",
    "\n",
    "    # Removing all unnecessary columbs, only keeping the 'features' and 'outcome' columns\n",
    "    stage_column_dropper = ColumnDropper(columns_to_drop = corelated_cols_to_remove+ ['vectorized_features', \"dataset\"])\n",
    "    # Connect the columns into a pipeline\n",
    "    pipeline = Pipeline(stages=[stage_typecaster,stage_outcome,stage_column_dropper])\n",
    "#     pipeline = Pipeline(stages=[stage_typecaster,stage_nominal_indexer,stage_nominal_onehot_encoder, stage_typecaster])\n",
    "    return pipeline \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6c3a6975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------\n",
      " tcp_time_delta             | 0.998867   \n",
      " tcp_len                    | 10         \n",
      " mqtt_conack_flags          | 0          \n",
      " mqtt_conack_flags_reserved | 0.0        \n",
      " mqtt_conack_flags_sp       | 0.0        \n",
      " mqtt_conack_val            | 0.0        \n",
      " mqtt_conflag_cleansess     | 0.0        \n",
      " mqtt_conflag_passwd        | 0.0        \n",
      " mqtt_conflag_qos           | 0.0        \n",
      " mqtt_conflag_reserved      | 0.0        \n",
      " mqtt_conflag_retain        | 0.0        \n",
      " mqtt_conflag_uname         | 0.0        \n",
      " mqtt_conflag_willflag      | 0.0        \n",
      " mqtt_dupflag               | 0.0        \n",
      " mqtt_kalive                | 0.0        \n",
      " mqtt_len                   | 8.0        \n",
      " mqtt_msgid                 | 0.0        \n",
      " mqtt_msgtype               | 3.0        \n",
      " mqtt_proto_len             | 0.0        \n",
      " mqtt_protoname             | 0          \n",
      " mqtt_qos                   | 0.0        \n",
      " mqtt_retain                | 0.0        \n",
      " mqtt_sub_qos               | 0.0        \n",
      " mqtt_suback_qos            | 0.0        \n",
      " mqtt_ver                   | 0.0        \n",
      " mqtt_willmsg               | 0.0        \n",
      " mqtt_willmsg_len           | 0.0        \n",
      " mqtt_willtopic             | 0.0        \n",
      " mqtt_willtopic_len         | 0.0        \n",
      " target                     | legitimate \n",
      " tcp_flags_decimal          | 24         \n",
      " mqtt_conflags_decimal      | 0          \n",
      " mqtt_hdrflags_decimal      | 48         \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train_updated.show(1, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5a072970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------\n",
      " tcp_time_delta             | 0.029854  \n",
      " tcp_len                    | 0         \n",
      " mqtt_conack_flags          | 0         \n",
      " mqtt_conack_flags_reserved | 0.0       \n",
      " mqtt_conack_flags_sp       | 0.0       \n",
      " mqtt_conack_val            | 0.0       \n",
      " mqtt_conflag_cleansess     | 0.0       \n",
      " mqtt_conflag_passwd        | 0.0       \n",
      " mqtt_conflag_qos           | 0.0       \n",
      " mqtt_conflag_reserved      | 0.0       \n",
      " mqtt_conflag_retain        | 0.0       \n",
      " mqtt_conflag_uname         | 0.0       \n",
      " mqtt_conflag_willflag      | 0.0       \n",
      " mqtt_dupflag               | 0.0       \n",
      " mqtt_kalive                | 0.0       \n",
      " mqtt_len                   | 0.0       \n",
      " mqtt_msgid                 | 0.0       \n",
      " mqtt_msgtype               | 0.0       \n",
      " mqtt_proto_len             | 0.0       \n",
      " mqtt_protoname             | 0         \n",
      " mqtt_qos                   | 0.0       \n",
      " mqtt_retain                | 0.0       \n",
      " mqtt_sub_qos               | 0.0       \n",
      " mqtt_suback_qos            | 0.0       \n",
      " mqtt_ver                   | 0.0       \n",
      " mqtt_willmsg               | 0.0       \n",
      " mqtt_willmsg_len           | 0.0       \n",
      " mqtt_willtopic             | 0.0       \n",
      " mqtt_willtopic_len         | 0.0       \n",
      " target                     | malformed \n",
      " tcp_flags_decimal          | 20        \n",
      " mqtt_conflags_decimal      | 0         \n",
      " mqtt_hdrflags_decimal      | 0         \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_updated.show(1, vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e7667ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline = get_preprocess_pipeline()\n",
    "preprocess_pipeline_model = preprocess_pipeline.fit(df_train_updated)\n",
    "\n",
    "df_train_final = preprocess_pipeline_model.transform(df_train_updated)\n",
    "df_test_final = preprocess_pipeline_model.transform(df_test_updated)\n",
    "\n",
    "\n",
    "df_train_pandas = df_train_final.toPandas()\n",
    "df_test_pandas = df_test_final.toPandas()\n",
    "# df_test_pandas.to_csv( \"/Users/sz904/Desktop/df_test_tf.csv\", index=False)\n",
    "# df_train_pandas.to_csv( \"/Users/sz904/Desktop/df_train_tf.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d70e87d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tcp_time_delta</th>\n",
       "      <th>tcp_len</th>\n",
       "      <th>mqtt_conack_val</th>\n",
       "      <th>mqtt_conflag_cleansess</th>\n",
       "      <th>mqtt_conflag_passwd</th>\n",
       "      <th>mqtt_conflag_uname</th>\n",
       "      <th>mqtt_dupflag</th>\n",
       "      <th>mqtt_kalive</th>\n",
       "      <th>mqtt_len</th>\n",
       "      <th>mqtt_msgid</th>\n",
       "      <th>mqtt_msgtype</th>\n",
       "      <th>mqtt_proto_len</th>\n",
       "      <th>mqtt_protoname</th>\n",
       "      <th>mqtt_qos</th>\n",
       "      <th>mqtt_retain</th>\n",
       "      <th>mqtt_ver</th>\n",
       "      <th>tcp_flags_decimal</th>\n",
       "      <th>mqtt_conflags_decimal</th>\n",
       "      <th>mqtt_hdrflags_decimal</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>5754.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999820</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99285</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99286</th>\n",
       "      <td>0.000120</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99287</th>\n",
       "      <td>0.001063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99288</th>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99289</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99290 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tcp_time_delta  tcp_len  mqtt_conack_val  mqtt_conflag_cleansess  \\\n",
       "0            0.029854      0.0              0.0                     0.0   \n",
       "1            0.000001     51.0              0.0                     0.0   \n",
       "2            0.999820     13.0              0.0                     0.0   \n",
       "3            0.007949      0.0              0.0                     0.0   \n",
       "4            0.000121      0.0              0.0                     0.0   \n",
       "...               ...      ...              ...                     ...   \n",
       "99285        0.000000      0.0              0.0                     0.0   \n",
       "99286        0.000120     14.0              0.0                     0.0   \n",
       "99287        0.001063      0.0              0.0                     0.0   \n",
       "99288        0.000026      0.0              0.0                     0.0   \n",
       "99289        0.000000      0.0              0.0                     0.0   \n",
       "\n",
       "       mqtt_conflag_passwd  mqtt_conflag_uname  mqtt_dupflag  mqtt_kalive  \\\n",
       "0                      0.0                 0.0           0.0          0.0   \n",
       "1                      0.0                 0.0           1.0          0.0   \n",
       "2                      0.0                 0.0           0.0          0.0   \n",
       "3                      0.0                 0.0           0.0          0.0   \n",
       "4                      0.0                 0.0           0.0          0.0   \n",
       "...                    ...                 ...           ...          ...   \n",
       "99285                  0.0                 0.0           0.0          0.0   \n",
       "99286                  0.0                 0.0           0.0          0.0   \n",
       "99287                  0.0                 0.0           0.0          0.0   \n",
       "99288                  0.0                 0.0           0.0          0.0   \n",
       "99289                  0.0                 0.0           0.0          0.0   \n",
       "\n",
       "       mqtt_len  mqtt_msgid  mqtt_msgtype  mqtt_proto_len  mqtt_protoname  \\\n",
       "0           0.0         0.0           0.0             0.0             0.0   \n",
       "1         171.0      5754.0           3.0             0.0             0.0   \n",
       "2          11.0         0.0           3.0             0.0             0.0   \n",
       "3           0.0         0.0           0.0             0.0             0.0   \n",
       "4           0.0         0.0           0.0             0.0             0.0   \n",
       "...         ...         ...           ...             ...             ...   \n",
       "99285       0.0         0.0           0.0             0.0             0.0   \n",
       "99286      12.0         0.0           3.0             0.0             0.0   \n",
       "99287       0.0         0.0           0.0             0.0             0.0   \n",
       "99288       0.0         0.0           0.0             0.0             0.0   \n",
       "99289       0.0         0.0           0.0             0.0             0.0   \n",
       "\n",
       "       mqtt_qos  mqtt_retain  mqtt_ver  tcp_flags_decimal  \\\n",
       "0           0.0          0.0       0.0               20.0   \n",
       "1           1.0          0.0       0.0               24.0   \n",
       "2           0.0          0.0       0.0               24.0   \n",
       "3           0.0          0.0       0.0               18.0   \n",
       "4           0.0          0.0       0.0               16.0   \n",
       "...         ...          ...       ...                ...   \n",
       "99285       0.0          0.0       0.0                2.0   \n",
       "99286       0.0          0.0       0.0               24.0   \n",
       "99287       0.0          0.0       0.0               16.0   \n",
       "99288       0.0          0.0       0.0               16.0   \n",
       "99289       0.0          0.0       0.0               16.0   \n",
       "\n",
       "       mqtt_conflags_decimal  mqtt_hdrflags_decimal  label  \n",
       "0                        0.0                    0.0      2  \n",
       "1                        0.0                   58.0      1  \n",
       "2                        0.0                   48.0      0  \n",
       "3                        0.0                    0.0      2  \n",
       "4                        0.0                    0.0      1  \n",
       "...                      ...                    ...    ...  \n",
       "99285                    0.0                    0.0      4  \n",
       "99286                    0.0                   48.0      0  \n",
       "99287                    0.0                    0.0      4  \n",
       "99288                    0.0                    0.0      1  \n",
       "99289                    0.0                    0.0      1  \n",
       "\n",
       "[99290 rows x 20 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d2ce08d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tcp_time_delta</th>\n",
       "      <th>tcp_len</th>\n",
       "      <th>mqtt_conack_val</th>\n",
       "      <th>mqtt_conflag_cleansess</th>\n",
       "      <th>mqtt_conflag_passwd</th>\n",
       "      <th>mqtt_conflag_uname</th>\n",
       "      <th>mqtt_dupflag</th>\n",
       "      <th>mqtt_kalive</th>\n",
       "      <th>mqtt_len</th>\n",
       "      <th>mqtt_msgid</th>\n",
       "      <th>mqtt_msgtype</th>\n",
       "      <th>mqtt_proto_len</th>\n",
       "      <th>mqtt_protoname</th>\n",
       "      <th>mqtt_qos</th>\n",
       "      <th>mqtt_retain</th>\n",
       "      <th>mqtt_ver</th>\n",
       "      <th>tcp_flags_decimal</th>\n",
       "      <th>mqtt_conflags_decimal</th>\n",
       "      <th>mqtt_hdrflags_decimal</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.998867</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000067</td>\n",
       "      <td>1460.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>2714.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000058</td>\n",
       "      <td>1460.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>1548.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000227</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000236</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231641</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231642</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231643</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231644</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6399.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231645</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>231646 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tcp_time_delta  tcp_len  mqtt_conack_val  mqtt_conflag_cleansess  \\\n",
       "0             0.998867     10.0              0.0                     0.0   \n",
       "1             0.000067   1460.0              0.0                     0.0   \n",
       "2             0.000058   1460.0              0.0                     0.0   \n",
       "3             0.000227     10.0              0.0                     0.0   \n",
       "4             0.000236     16.0              0.0                     0.0   \n",
       "...                ...      ...              ...                     ...   \n",
       "231641        0.000003      0.0              0.0                     0.0   \n",
       "231642        0.000003      0.0              0.0                     0.0   \n",
       "231643        0.000010      0.0              0.0                     0.0   \n",
       "231644        0.000001     12.0              0.0                     0.0   \n",
       "231645        0.000006      0.0              0.0                     0.0   \n",
       "\n",
       "        mqtt_conflag_passwd  mqtt_conflag_uname  mqtt_dupflag  mqtt_kalive  \\\n",
       "0                       0.0                 0.0           0.0          0.0   \n",
       "1                       0.0                 0.0           0.0          0.0   \n",
       "2                       0.0                 0.0           0.0          0.0   \n",
       "3                       0.0                 0.0           0.0          0.0   \n",
       "4                       0.0                 0.0           0.0          0.0   \n",
       "...                     ...                 ...           ...          ...   \n",
       "231641                  0.0                 0.0           0.0          0.0   \n",
       "231642                  0.0                 0.0           0.0          0.0   \n",
       "231643                  0.0                 0.0           0.0          0.0   \n",
       "231644                  0.0                 0.0           0.0          0.0   \n",
       "231645                  0.0                 0.0           0.0          0.0   \n",
       "\n",
       "        mqtt_len  mqtt_msgid  mqtt_msgtype  mqtt_proto_len  mqtt_protoname  \\\n",
       "0            8.0         0.0           3.0             0.0             0.0   \n",
       "1          169.0      2714.0           3.0             0.0             0.0   \n",
       "2          163.0      1548.0           3.0             0.0             0.0   \n",
       "3            8.0         0.0           3.0             0.0             0.0   \n",
       "4            2.0      2800.0           4.0             0.0             0.0   \n",
       "...          ...         ...           ...             ...             ...   \n",
       "231641       0.0         0.0           0.0             0.0             0.0   \n",
       "231642       0.0         0.0           0.0             0.0             0.0   \n",
       "231643       0.0         0.0           0.0             0.0             0.0   \n",
       "231644       2.0      6399.0           4.0             0.0             0.0   \n",
       "231645       0.0         0.0           0.0             0.0             0.0   \n",
       "\n",
       "        mqtt_qos  mqtt_retain  mqtt_ver  tcp_flags_decimal  \\\n",
       "0            0.0          0.0       0.0               24.0   \n",
       "1            1.0          0.0       0.0               16.0   \n",
       "2            1.0          0.0       0.0               16.0   \n",
       "3            0.0          0.0       0.0               24.0   \n",
       "4            0.0          0.0       0.0               24.0   \n",
       "...          ...          ...       ...                ...   \n",
       "231641       0.0          0.0       0.0               16.0   \n",
       "231642       0.0          0.0       0.0               16.0   \n",
       "231643       0.0          0.0       0.0               16.0   \n",
       "231644       0.0          0.0       0.0               24.0   \n",
       "231645       0.0          0.0       0.0               16.0   \n",
       "\n",
       "        mqtt_conflags_decimal  mqtt_hdrflags_decimal  label  \n",
       "0                         0.0                   48.0      0  \n",
       "1                         0.0                   50.0      1  \n",
       "2                         0.0                   50.0      1  \n",
       "3                         0.0                   48.0      0  \n",
       "4                         0.0                   64.0      1  \n",
       "...                       ...                    ...    ...  \n",
       "231641                    0.0                    0.0      0  \n",
       "231642                    0.0                    0.0      0  \n",
       "231643                    0.0                    0.0      0  \n",
       "231644                    0.0                   64.0      1  \n",
       "231645                    0.0                    0.0      0  \n",
       "\n",
       "[231646 rows x 20 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ba7d1d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tcp_time_delta: float (nullable = true)\n",
      " |-- tcp_len: float (nullable = true)\n",
      " |-- mqtt_conack_val: float (nullable = true)\n",
      " |-- mqtt_conflag_cleansess: float (nullable = true)\n",
      " |-- mqtt_conflag_passwd: float (nullable = true)\n",
      " |-- mqtt_conflag_uname: float (nullable = true)\n",
      " |-- mqtt_dupflag: float (nullable = true)\n",
      " |-- mqtt_kalive: float (nullable = true)\n",
      " |-- mqtt_len: float (nullable = true)\n",
      " |-- mqtt_msgid: float (nullable = true)\n",
      " |-- mqtt_msgtype: float (nullable = true)\n",
      " |-- mqtt_proto_len: float (nullable = true)\n",
      " |-- mqtt_protoname: float (nullable = true)\n",
      " |-- mqtt_qos: float (nullable = true)\n",
      " |-- mqtt_retain: float (nullable = true)\n",
      " |-- mqtt_ver: float (nullable = true)\n",
      " |-- tcp_flags_decimal: float (nullable = true)\n",
      " |-- mqtt_conflags_decimal: float (nullable = true)\n",
      " |-- mqtt_hdrflags_decimal: float (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "84c90519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# from pyspark.ml.classification import RandomForestClassifier\n",
    "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# from pyspark.ml.classification import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8ce887e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxBins = 8)\n",
    "# rf_model = rf.fit(df_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4960d8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf  # now import the tensorflow module\n",
    "print(tf.__version__)  # make sure the version is 2.x\n",
    "# from pyspark.sql.functions import udf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2ae01c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_tf = pd.read_csv(\"/Users/sz904/Desktop/df_train_tf.csv\")\n",
    "# df_test_tf = pd.read_csv(\"/Users/sz904/Desktop/df_test_tf.csv\")\n",
    "\n",
    "df_train_tf = df_train_pandas\n",
    "df_test_tf = df_test_pandas\n",
    "\n",
    "x_valid, x_test, y_valid, y_test = train_test_split( df_test_tf.iloc[:, 0:-1], df_test_tf.iloc[:, -1:], test_size=0.33, random_state=42)\n",
    "\n",
    "df_train_tf = df_train_tf.iloc[:100000, :]\n",
    "\n",
    "x_train = df_train_tf.iloc[:, 0:-1]\n",
    "y_train = df_train_tf.iloc[:, -1:]\n",
    "y_train=np.ravel(y_train)\n",
    "\n",
    "\n",
    "\n",
    "# x_valid = df_valid.iloc[:, 0:-1]\n",
    "# y_valid = df_valid.iloc[:, -1:]\n",
    "# y_valid=np.ravel(y_valid)\n",
    "\n",
    "# x_test =  df_test.iloc[:, 0:-1]\n",
    "# y_test = df_test.iloc[:, -1:]\n",
    "# y_test=np.ravel(y_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_valid = scaler.transform(x_valid)\n",
    "\n",
    "# x_train = tf.constant(np.array(df_train_pandas['features'].values.tolist()))\n",
    "# y_train = tf.constant(np.array(df_train_pandas['label'].values.tolist()))\n",
    "\n",
    "# x_validate = tf.constant(np.array(df_valid_pandas['features'].values.tolist()))\n",
    "# y_validate = tf.constant(np.array(df_valid_pandas['label'].values.tolist()))\n",
    "\n",
    "\n",
    "# x_test = tf.constant(np.array(df_test_pandas['features'].values.tolist()))\n",
    "# y_test = tf.constant(np.array(df_test_pandas['label'].values.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c08545c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2544472 , -0.1518746 , -0.06700799, ...,  0.82465214,\n",
       "        -0.06663252,  0.4842833 ],\n",
       "       [-0.09625363,  1.4371164 , -0.06700799, ..., -0.95063066,\n",
       "        -0.06663252,  0.55086243],\n",
       "       [-0.09625679,  1.4371164 , -0.06700799, ..., -0.95063066,\n",
       "        -0.06663252,  0.55086243],\n",
       "       ...,\n",
       "       [-0.09624975,  1.4371164 , -0.06700799, ..., -0.95063066,\n",
       "        -0.06663252,  0.55086243],\n",
       "       [-0.09627469, -0.14858703, -0.06700799, ...,  0.82465214,\n",
       "        -0.06663252,  0.4842833 ],\n",
       "       [-0.09620552, -0.15406631, -0.06700799, ...,  0.82465214,\n",
       "        -0.06663252,  1.0169164 ]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "560f740a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09623852, -0.14858703, -0.06700799, ...,  0.82465214,\n",
       "        -0.06663252,  0.4842833 ],\n",
       "       [-0.09627399, -0.16283315, -0.06700799, ..., -0.95063066,\n",
       "        -0.06663252, -1.1136159 ],\n",
       "       [-0.09627539, -0.16283315, -0.06700799, ..., -0.95063066,\n",
       "        -0.06663252, -1.1136159 ],\n",
       "       ...,\n",
       "       [ 0.25482818, -0.1518746 , -0.06700799, ...,  0.82465214,\n",
       "        -0.06663252,  0.4842833 ],\n",
       "       [-0.09624343,  1.4371164 , -0.06700799, ..., -0.95063066,\n",
       "        -0.06663252,  0.55086243],\n",
       "       [ 0.25478113, -0.14858703, -0.06700799, ...,  0.82465214,\n",
       "        -0.06663252,  0.4842833 ]], dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d597b7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2548243 , -0.14749117, -0.06700799, ...,  0.82465214,\n",
       "        -0.06663252,  0.4842833 ],\n",
       "       [-0.09622483, -0.14749117, -0.06700799, ...,  0.82465214,\n",
       "        -0.06663252,  0.4842833 ],\n",
       "       [-0.09627715, -0.07735639, -0.06700799, ...,  0.82465214,\n",
       "        -0.06663252,  0.81717896],\n",
       "       ...,\n",
       "       [-0.09627679, -0.0894108 , -0.06700799, ...,  0.82465214,\n",
       "        -0.06663252,  0.81717896],\n",
       "       [-0.09627715, -0.16283315, -0.06700799, ..., -0.95063066,\n",
       "        -0.06663252, -1.1136159 ],\n",
       "       [-0.09627469, -0.16283315, -0.06700799, ..., -0.95063066,\n",
       "        -0.06663252, -1.1136159 ]], dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d01fdf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_decision_forests as tfdf\n",
    "# import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cc6e7f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_1 = tfdf.keras.RandomForestModel(verbose=2)\n",
    "# model_1.fit(x=df_train_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3694c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential( [keras.layers.Dense(10,activation='relu'),\n",
    "                           keras.layers.Dense(10,activation='tanh'),\n",
    "\n",
    "                           keras.layers.Dense(10,activation='tanh') ,\n",
    "                           keras.layers.BatchNormalization(epsilon=1e-06,  momentum=0.9, weights=None),\n",
    "                           keras.layers.Dense(6)] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9c936fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 22:03:45.351656: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-11-26 22:03:58.996122: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 - 22s - loss: 1.1090 - Accuracy_epochs: 0.4952 - val_loss: 1.0759 - val_Accuracy_epochs: 0.5015 - 22s/epoch - 7ms/step\n",
      "Epoch 2/15\n",
      "3125/3125 - 21s - loss: 1.0749 - Accuracy_epochs: 0.4996 - val_loss: 1.0745 - val_Accuracy_epochs: 0.5015 - 21s/epoch - 7ms/step\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [127], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD(),\n\u001b[1;32m      4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      5\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m [keras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mSparseCategoricalAccuracy(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[1;32m      7\u001b[0m tensorboard_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(log_dir\u001b[38;5;241m=\u001b[39mlog_dir_Q1_2, histogram_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/14813project/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/14813project/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniforge3/envs/14813project/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/14813project/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniforge3/envs/14813project/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniforge3/envs/14813project/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/14813project/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniforge3/envs/14813project/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniforge3/envs/14813project/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log_dir_Q1_2 = \"/Users/sz904/Desktop/\"\n",
    "\n",
    "model.compile(optimizer = keras.optimizers.SGD(),\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics = [keras.metrics.SparseCategoricalAccuracy(name=\"Accuracy_epochs\")])\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_Q1_2, histogram_freq=1)\n",
    "\n",
    "model.fit(x=x_train, y=y_train, \n",
    "          epochs=15, verbose = 2,\n",
    "          validation_data=(x_valid, y_valid), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed7d6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83e56afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 16:21:45 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "22/11/26 16:21:45 WARN DAGScheduler: Broadcasting large task binary with size 16.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=====>                                                    (1 + 9) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 16:21:52 ERROR Executor: Exception in task 1.0 in stage 7.0 (TID 29)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3481)\n",
      "\tat java.base/java.util.ArrayList.trimToSize(ArrayList.java:204)\n",
      "\tat net.razorvine.pickle.Unpickler.load_appends(Unpickler.java:727)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:232)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$Lambda$4389/0x000000080207b4d8.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2639/0x0000000801d0f340.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2636/0x0000000801d0e908.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2598/0x0000000801d02130.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "22/11/26 16:21:53 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#102,Executor task launch worker for task 1.0 in stage 7.0 (TID 29),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3481)\n",
      "\tat java.base/java.util.ArrayList.trimToSize(ArrayList.java:204)\n",
      "\tat net.razorvine.pickle.Unpickler.load_appends(Unpickler.java:727)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:232)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$Lambda$4389/0x000000080207b4d8.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2639/0x0000000801d0f340.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2636/0x0000000801d0e908.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2598/0x0000000801d02130.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "22/11/26 16:21:53 WARN TaskSetManager: Lost task 1.0 in stage 7.0 (TID 29) (stevens-mbp-4 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3481)\n",
      "\tat java.base/java.util.ArrayList.trimToSize(ArrayList.java:204)\n",
      "\tat net.razorvine.pickle.Unpickler.load_appends(Unpickler.java:727)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:232)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$Lambda$4389/0x000000080207b4d8.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2639/0x0000000801d0f340.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2636/0x0000000801d0e908.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2598/0x0000000801d02130.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "\n",
      "22/11/26 16:21:53 ERROR TaskSetManager: Task 1 in stage 7.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=====>                                                    (1 + 8) / 10]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o661.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 1 times, most recent failure: Lost task 1.0 in stage 7.0 (TID 29) (stevens-mbp-4 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOf(Arrays.java:3481)\n\tat java.base/java.util.ArrayList.trimToSize(ArrayList.java:204)\n\tat net.razorvine.pickle.Unpickler.load_appends(Unpickler.java:727)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:232)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$Lambda$4389/0x000000080207b4d8.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2639/0x0000000801d0f340.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD$$Lambda$2636/0x0000000801d0e908.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2598/0x0000000801d02130.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3688)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3685)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOf(Arrays.java:3481)\n\tat java.base/java.util.ArrayList.trimToSize(ArrayList.java:204)\n\tat net.razorvine.pickle.Unpickler.load_appends(Unpickler.java:727)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:232)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$Lambda$4389/0x000000080207b4d8.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2639/0x0000000801d0f340.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD$$Lambda$2636/0x0000000801d0e908.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2598/0x0000000801d02130.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m df_test_temp, df_abandon1 \u001b[38;5;241m=\u001b[39m df_test_final\u001b[38;5;241m.\u001b[39mrandomSplit([\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m0.8\u001b[39m])\n\u001b[1;32m      5\u001b[0m df_valid,df_test \u001b[38;5;241m=\u001b[39m df_test_temp\u001b[38;5;241m.\u001b[39mrandomSplit([\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m0.5\u001b[39m])\n\u001b[0;32m----> 7\u001b[0m df_train_pandas \u001b[38;5;241m=\u001b[39m \u001b[43mdf_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m df_valid_pandas \u001b[38;5;241m=\u001b[39m df_valid\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, to_array(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[1;32m      9\u001b[0m df_test_pandas \u001b[38;5;241m=\u001b[39m df_test\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, to_array(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mtoPandas()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:205\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    206\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    208\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:817\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \n\u001b[1;32m    809\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m--> 817\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o661.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 1 times, most recent failure: Lost task 1.0 in stage 7.0 (TID 29) (stevens-mbp-4 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOf(Arrays.java:3481)\n\tat java.base/java.util.ArrayList.trimToSize(ArrayList.java:204)\n\tat net.razorvine.pickle.Unpickler.load_appends(Unpickler.java:727)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:232)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$Lambda$4389/0x000000080207b4d8.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2639/0x0000000801d0f340.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD$$Lambda$2636/0x0000000801d0e908.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2598/0x0000000801d02130.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3688)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3685)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOf(Arrays.java:3481)\n\tat java.base/java.util.ArrayList.trimToSize(ArrayList.java:204)\n\tat net.razorvine.pickle.Unpickler.load_appends(Unpickler.java:727)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:232)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$Lambda$4389/0x000000080207b4d8.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2639/0x0000000801d0f340.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD$$Lambda$2636/0x0000000801d0e908.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2598/0x0000000801d02130.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/26 16:21:56 WARN PythonUDFRunner: Incomplete task 3.0 in stage 7 (TID 31) interrupted: Attempting to kill Python Worker\n",
      "22/11/26 16:21:56 WARN PythonUDFRunner: Incomplete task 6.0 in stage 7 (TID 34) interrupted: Attempting to kill Python Worker\n",
      "22/11/26 16:21:56 WARN PythonUDFRunner: Incomplete task 2.0 in stage 7 (TID 30) interrupted: Attempting to kill Python Worker\n",
      "22/11/26 16:21:56 WARN PythonUDFRunner: Incomplete task 3.0 in stage 7 (TID 31) interrupted: Attempting to kill Python Worker\n",
      "22/11/26 16:21:56 WARN PythonUDFRunner: Incomplete task 4.0 in stage 7 (TID 32) interrupted: Attempting to kill Python Worker\n",
      "22/11/26 16:21:56 WARN PythonUDFRunner: Incomplete task 7.0 in stage 7 (TID 35) interrupted: Attempting to kill Python Worker\n",
      "22/11/26 16:21:56 WARN PythonUDFRunner: Incomplete task 5.0 in stage 7 (TID 33) interrupted: Attempting to kill Python Worker\n",
      "22/11/26 16:21:56 WARN PythonUDFRunner: Incomplete task 4.0 in stage 7 (TID 32) interrupted: Attempting to kill Python Worker\n",
      "22/11/26 16:21:56 WARN PythonUDFRunner: Incomplete task 8.0 in stage 7 (TID 36) interrupted: Attempting to kill Python Worker\n",
      "22/11/26 16:21:56 WARN PythonUDFRunner: Incomplete task 6.0 in stage 7 (TID 34) interrupted: Attempting to kill Python Worker\n",
      "22/11/26 16:21:57 WARN PythonUDFRunner: Incomplete task 0.0 in stage 7 (TID 28) interrupted: Attempting to kill Python Worker\n",
      "22/11/26 16:21:58 WARN PythonUDFRunner: Incomplete task 7.0 in stage 7 (TID 35) interrupted: Attempting to kill Python Worker\n",
      "22/11/26 16:21:58 WARN PythonUDFRunner: Incomplete task 5.0 in stage 7 (TID 33) interrupted: Attempting to kill Python Worker\n",
      "22/11/26 16:21:58 WARN PythonUDFRunner: Incomplete task 8.0 in stage 7 (TID 36) interrupted: Attempting to kill Python Worker\n",
      "22/11/26 16:21:58 WARN PythonUDFRunner: Incomplete task 2.0 in stage 7 (TID 30) interrupted: Attempting to kill Python Worker\n",
      "22/11/26 16:21:58 WARN PythonUDFRunner: Incomplete task 0.0 in stage 7 (TID 28) interrupted: Attempting to kill Python Worker\n",
      "22/11/26 16:22:01 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "org.apache.spark.SparkException: Block broadcast_12 does not exist\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.blockDoesNotExistError(SparkCoreErrors.scala:234)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.blockInfo(BlockInfoManager.scala:237)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:358)\n",
      "\tat org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:1291)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$releaseBlockManagerLock$1(TorrentBroadcast.scala:289)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$releaseBlockManagerLock$1$adapted(TorrentBroadcast.scala:289)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:131)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:143)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:143)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:198)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:143)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:136)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:146)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "22/11/26 16:22:01 ERROR Utils: Uncaught exception in thread Executor task launch worker for task 5.0 in stage 7.0 (TID 33)\n",
      "java.lang.NullPointerException: Cannot invoke \"org.apache.spark.SparkEnv.blockManager()\" because the return value of \"org.apache.spark.SparkEnv$.get()\" is null\n",
      "\tat org.apache.spark.scheduler.Task.$anonfun$run$3(Task.scala:157)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:155)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 50843)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sz904/miniforge3/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/sz904/miniforge3/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/sz904/miniforge3/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/sz904/miniforge3/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/sz904/miniforge3/lib/python3.10/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/sz904/miniforge3/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/Users/sz904/miniforge3/lib/python3.10/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/sz904/miniforge3/lib/python3.10/site-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "to_array = udf(lambda v: v.toArray().tolist(), ArrayType(FloatType()))\n",
    "\n",
    "# df_train, df_abandon = df_train_final.randomSplit([0.01,0.99])\n",
    "# df_test_temp, df_abandon1 = df_test_final.randomSplit([0.2,0.8])\n",
    "df_valid,df_test = df_test_temp.randomSplit([0.5,0.5])\n",
    "\n",
    "df_train_pandas = df_train.withColumn('features', to_array('features')).toPandas()\n",
    "df_valid_pandas = df_valid.withColumn('features', to_array('features')).toPandas()\n",
    "df_test_pandas = df_test.withColumn('features', to_array('features')).toPandas()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
